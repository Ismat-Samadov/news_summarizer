name: Scrape News Articles

on:
  # Scheduled scraping - 3 times daily
  schedule:
    - cron: '0 9 * * *'   # 09:00 UTC daily
    - cron: '0 13 * * *'  # 13:00 UTC daily
    - cron: '0 18 * * *'  # 18:00 UTC daily

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape per source'
        required: false
        default: '3'
      scrape_details:
        description: 'Scrape full article content (true/false)'
        required: false
        default: 'false'

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Create logs directory
        run: mkdir -p logs

      - name: Run scraper for all sources
        env:
          DATABASE: ${{ secrets.DATABASE }}
          DATABASE_URL: ${{ secrets.DATABASE }}
          USE_PLAYWRIGHT: 'true'
        run: |
          python -m scraper_job.run_scraper run-all \
            --pages ${{ github.event.inputs.max_pages || '3' }} \
            ${{ github.event.inputs.scrape_details == 'true' && '--details' || '' }} \
            --triggered-by github_action

      - name: Display statistics
        if: always()
        env:
          DATABASE: ${{ secrets.DATABASE }}
          DATABASE_URL: ${{ secrets.DATABASE }}
        run: |
          python -m scraper_job.run_scraper stats

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: logs/
          retention-days: 7

      - name: Notify on failure
        if: failure()
        run: |
          echo "::error::Scraping job failed. Check the logs for details."
          exit 1
