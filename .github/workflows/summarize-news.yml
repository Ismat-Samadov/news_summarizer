name: Generate AI Summaries

on:
  # Run after scraping jobs (with delay)
  schedule:
    - cron: '30 9 * * *'   # 09:30 UTC daily (30 min after scraping)
    - cron: '30 13 * * *'  # 13:30 UTC daily
    - cron: '30 18 * * *'  # 18:30 UTC daily

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of articles to summarize'
        required: false
        default: '100'

jobs:
  summarize:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create logs directory
        run: mkdir -p logs

      - name: Generate AI summaries
        env:
          DATABASE: ${{ secrets.DATABASE }}
          DATABASE_URL: ${{ secrets.DATABASE }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python -m scraper_job.run_summarizer run \
            --batch-size ${{ github.event.inputs.batch_size || '100' }} \
            --triggered-by github_action

      - name: Display statistics
        if: always()
        env:
          DATABASE: ${{ secrets.DATABASE }}
          DATABASE_URL: ${{ secrets.DATABASE }}
        run: |
          python -m scraper_job.run_scraper stats

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: summarization-logs-${{ github.run_number }}
          path: logs/
          retention-days: 7
